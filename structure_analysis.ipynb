{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist, pos_tag\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./data/json_pr.json') as x:\n",
    "    prj = json.load(x)\n",
    "    \n",
    "with open ('./data/json_ra.json') as y:\n",
    "    raj = json.load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj = pd.DataFrame(prj)\n",
    "df_raj = pd.DataFrame(raj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #most up-to-date files have 0 nulls.  confirmed\n",
    "# print(df_prj.isnull().sum())\n",
    "# print(df_raj.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure analysis separate from diction:\n",
    "\n",
    "Number of sentences, sentence length, and *rate* of punctuation use (. , ?) could be informative.  How preserve?  \n",
    "\n",
    "2 separate analyses: nlp based on Bag of Words (0 structure or punctuation retention), and one based on structure.  \n",
    "\n",
    "Word Frequencies (see lesson 5.04) counts *everything* - punctuation, numbers, etc.  But I'd rather use WordFreq, CountVec, etc for lexicon and reduce punctuation down to counts.\n",
    "\n",
    "New columns for total wordcount, # sentences, average sentence length, total commas / ? / ! (which can be changed into per wordcount or per sentence 'rates' later).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with sentence count\n",
    "Methods based on experimentation in scratch notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj['num_sentences'] = df_prj['selftext'].map(lambda x: len(re.split(r'[.!?]+', x))-1)\n",
    "df_raj['num_sentences'] = df_raj['selftext'].map(lambda x: len(re.split(r'[.!?]+', x))-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with average sentence length (by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len counts spaces and characters.  you want wordcount.\n",
    "length = 'this is twenty lettersS..!?,'\n",
    "len(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raj_mini['words_per_sentence'] = \n",
    "\n",
    "def avg_words_per_sentence(df):\n",
    "    \n",
    "    all_sentences_bypost = []\n",
    "\n",
    "    for i in df['selftext']:\n",
    "        all_sentences_bypost.append(re.split(r'[.!?]+', i))\n",
    "    \n",
    "    avgwordsper = []\n",
    "\n",
    "    for post in all_sentences_bypost:\n",
    "        wordspersentence = []\n",
    "        for i in post:\n",
    "            wordspersentence.append(len(i.split()))\n",
    "        avgwordsper.append(int(np.mean(wordspersentence).round()))\n",
    "        \n",
    "    return avgwordsper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['avg_sentence_len'] = avg_words_per_sentence(df_raj)\n",
    "df_prj['avg_sentence_len'] = avg_words_per_sentence(df_prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns with total wordcount, commas, ?s, and !s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['wordcount'] = df_raj['selftext'].map(lambda x: len(x.split()))\n",
    "df_prj['wordcount'] = df_prj['selftext'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['comma_count'] = df_raj['selftext'].map(lambda x: x.count(','))\n",
    "df_raj['qmark_count'] = df_raj['selftext'].map(lambda x: x.count('?'))\n",
    "df_raj['exclamatios'] = df_raj['selftext'].map(lambda x: x.count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punc(punc, df):\n",
    "    perpost = []\n",
    "    for i in df['selftext']:\n",
    "        perpost.append(i.count(punc))\n",
    "    return perpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj['comma_count'] = count_punc(',', df_prj)\n",
    "df_prj['qmark_count'] = count_punc('?', df_prj)\n",
    "df_prj['exclamatios'] = count_punc('!', df_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No. Use total comma count, ! count, ? count\n",
    "## Can turn into 'rate' later.  Prob by sentence count, not wordcount...\n",
    "\n",
    "# def punc_rate(punctuation, df):\n",
    "    \n",
    "#     puncs=[]\n",
    "#     punc_rate=[]\n",
    "    \n",
    "#     for i in df['selftext']:\n",
    "#         commas.append(len(re.findall(punctuation, i)))\n",
    "    \n",
    "#     for j in commas:\n",
    "#         comma_rate.append(commas[j]/df['wordcount'][j])\n",
    "        \n",
    "#     return punc_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raj['comma_rate'] = punc_rate(',', df_raj)\n",
    "# df_prj['comma_rate'] = punc_rate(',', df_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overly complicated just use map + str.count\n",
    "# run into issues with 0s with re.findall I think\n",
    "\n",
    "# def punc_count(punc, df):\n",
    "#     count = []\n",
    "#     for i in df['selftext']:\n",
    "#         count.append(len(re.findall(punc, i)))\n",
    "#     return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special characters to scrub:\n",
    "* characters with alphanum attached: I think remove these whole units first (or the attached text), otherwise left with 'n', 'amp', 'u username', etc that won't be picked up by subsequent alphanum filter\n",
    "    * ' & amp ;\n",
    "    * \\n\n",
    "    * u/username\n",
    "    * (Mobile)\n",
    "        * edit: leave mobile in place, could be differentiating feature between subreddits\n",
    "* all non-alpha-numerics: \\, /, &, * , \"\", '', -\n",
    "* emojis\n",
    "* any URLs:  start with https://...  \n",
    "* I don't think numbers will be informative\n",
    "* leave punctuation in place?  ADHD-comma anecdote, prevalence of ! ? might be informative?\n",
    "    * RA likely contains ? all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prj.loc[df_prj['selftext'].str.contains('mobile|Mobile')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raj.loc[df_raj['selftext'].str.contains('mobile|Mobile')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sorry',\n",
       " 'in',\n",
       " 'advance',\n",
       " 'if',\n",
       " 'my',\n",
       " 'texts',\n",
       " 'are',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'sloppy..',\n",
       " \"It's\",\n",
       " 'taken',\n",
       " 'me',\n",
       " 'a',\n",
       " 'long',\n",
       " 'while',\n",
       " 'to',\n",
       " 'finally',\n",
       " 'write',\n",
       " 'this',\n",
       " 'out',\n",
       " 'as',\n",
       " \"I'm\",\n",
       " 'just',\n",
       " 'very',\n",
       " 'unsure.',\n",
       " 'Side',\n",
       " 'notes:',\n",
       " \"I'm\",\n",
       " 'not',\n",
       " 'from',\n",
       " 'USA.',\n",
       " \"I'm\",\n",
       " 'from',\n",
       " 'Asia',\n",
       " '&amp;',\n",
       " 'My',\n",
       " 'country',\n",
       " 'to',\n",
       " 'hers',\n",
       " 'is',\n",
       " 'about',\n",
       " '30m',\n",
       " 'flight',\n",
       " '&amp;',\n",
       " 'cheap.',\n",
       " 'Backstory;',\n",
       " 'When',\n",
       " 'I',\n",
       " 'was',\n",
       " '14,',\n",
       " 'I',\n",
       " 'had',\n",
       " 'my',\n",
       " 'first',\n",
       " 'ever',\n",
       " \"'real'\",\n",
       " 'relationship.',\n",
       " 'At',\n",
       " 'the',\n",
       " 'time',\n",
       " 'it',\n",
       " 'was',\n",
       " 'solely',\n",
       " 'online',\n",
       " 'until',\n",
       " 'I',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'over',\n",
       " 'to',\n",
       " 'her',\n",
       " 'country',\n",
       " '(accompanied',\n",
       " 'by',\n",
       " 'my',\n",
       " \"mom's\",\n",
       " 'friend)',\n",
       " 'twice',\n",
       " 'a',\n",
       " 'month',\n",
       " 'when',\n",
       " 'I',\n",
       " 'was',\n",
       " '15.',\n",
       " 'Before',\n",
       " 'you',\n",
       " 'hammer',\n",
       " 'about',\n",
       " 'my',\n",
       " 'parents',\n",
       " 'not',\n",
       " 'seeing',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'in',\n",
       " 'our',\n",
       " 'age',\n",
       " 'gap,',\n",
       " 'she',\n",
       " 'always',\n",
       " 'made',\n",
       " 'me',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'that',\n",
       " 'we',\n",
       " 'were',\n",
       " 'only',\n",
       " 'a',\n",
       " 'year',\n",
       " 'apart',\n",
       " 'because',\n",
       " 'according',\n",
       " 'to',\n",
       " 'her,',\n",
       " '\"they',\n",
       " \"wouldn't\",\n",
       " 'understand',\n",
       " 'our',\n",
       " 'love\".',\n",
       " 'Looking',\n",
       " 'back,',\n",
       " 'I',\n",
       " 'admit',\n",
       " 'I',\n",
       " 'was',\n",
       " 'an',\n",
       " 'idiot',\n",
       " 'for',\n",
       " 'not',\n",
       " 'seeing',\n",
       " 'how',\n",
       " 'a',\n",
       " '15',\n",
       " 'year',\n",
       " 'old',\n",
       " '&amp;',\n",
       " 'a',\n",
       " '18',\n",
       " 'year',\n",
       " 'old',\n",
       " 'being',\n",
       " 'in',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'was',\n",
       " 'not',\n",
       " 'only',\n",
       " 'a',\n",
       " 'red',\n",
       " 'flag',\n",
       " 'but',\n",
       " 'also',\n",
       " 'well..',\n",
       " 'illegal.',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'my',\n",
       " 'rational',\n",
       " 'thinking',\n",
       " 'was',\n",
       " 'put',\n",
       " 'aside',\n",
       " 'because',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " '(still',\n",
       " 'am)',\n",
       " 'insecure',\n",
       " 'kid',\n",
       " 'and',\n",
       " 'to',\n",
       " 'be',\n",
       " 'approached',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'a',\n",
       " 'kind',\n",
       " 'and',\n",
       " 'good',\n",
       " 'looking',\n",
       " 'girl',\n",
       " 'was',\n",
       " 'just',\n",
       " 'amazing',\n",
       " 'for',\n",
       " 'me.',\n",
       " 'I',\n",
       " 'always',\n",
       " 'told',\n",
       " 'her',\n",
       " '(because',\n",
       " 'of',\n",
       " 'my',\n",
       " 'upbringing)',\n",
       " 'that',\n",
       " 'sex',\n",
       " 'was',\n",
       " 'completely',\n",
       " 'off',\n",
       " 'the',\n",
       " 'table.',\n",
       " 'I',\n",
       " 'just',\n",
       " \"wasn't\",\n",
       " 'into',\n",
       " 'it',\n",
       " 'after',\n",
       " 'finding',\n",
       " 'out',\n",
       " 'about',\n",
       " 'pregnancies',\n",
       " 'and',\n",
       " 'all',\n",
       " 'that,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'terrifying',\n",
       " 'to',\n",
       " 'me',\n",
       " 'at',\n",
       " 'that',\n",
       " 'age.',\n",
       " '(We',\n",
       " \"aren't\",\n",
       " 'taught',\n",
       " 'sex-ed',\n",
       " 'in',\n",
       " 'school',\n",
       " 'so',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'any',\n",
       " 'better',\n",
       " 'about',\n",
       " 'protection',\n",
       " 'all',\n",
       " 'that.).',\n",
       " 'This',\n",
       " 'was',\n",
       " 'my',\n",
       " 'first',\n",
       " 'every',\n",
       " 'relationship,',\n",
       " 'and',\n",
       " 'according',\n",
       " 'to',\n",
       " 'her,',\n",
       " 'her',\n",
       " '3rd(?)',\n",
       " 'So',\n",
       " 'I',\n",
       " 'was',\n",
       " 'aware',\n",
       " 'she',\n",
       " 'was',\n",
       " 'way',\n",
       " 'more',\n",
       " 'experienced',\n",
       " 'than',\n",
       " 'I',\n",
       " 'was.',\n",
       " 'The',\n",
       " 'incident',\n",
       " 'happened',\n",
       " 'when',\n",
       " 'she',\n",
       " 'invited',\n",
       " 'me',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'the',\n",
       " 'night',\n",
       " 'at',\n",
       " 'her',\n",
       " 'place',\n",
       " '(I',\n",
       " 'was',\n",
       " 'staying',\n",
       " 'in',\n",
       " 'a',\n",
       " 'hotel,',\n",
       " 'in',\n",
       " 'my',\n",
       " 'own',\n",
       " 'room',\n",
       " 'separate',\n",
       " 'from',\n",
       " \"mom's\",\n",
       " 'friend)',\n",
       " 'since',\n",
       " 'her',\n",
       " 'parents',\n",
       " \"weren't\",\n",
       " 'around',\n",
       " '&amp;',\n",
       " 'I',\n",
       " 'agreed.',\n",
       " 'After',\n",
       " 'watching',\n",
       " 'movies',\n",
       " 'and',\n",
       " 'playing',\n",
       " 'games',\n",
       " 'on',\n",
       " 'her',\n",
       " 'computer',\n",
       " 'and',\n",
       " 'such',\n",
       " 'we',\n",
       " 'went',\n",
       " 'to',\n",
       " 'bed.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'tired',\n",
       " 'so',\n",
       " 'just',\n",
       " 'a',\n",
       " 'goodnight',\n",
       " 'peck',\n",
       " 'and',\n",
       " 'sleep.',\n",
       " 'She',\n",
       " 'kept',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'engage',\n",
       " 'in',\n",
       " 'sexual',\n",
       " 'activity',\n",
       " 'but',\n",
       " 'I',\n",
       " 'kept',\n",
       " 'refusing',\n",
       " 'because',\n",
       " 'as',\n",
       " 'said',\n",
       " 'above,',\n",
       " 'I',\n",
       " 'really',\n",
       " 'just',\n",
       " \"didn't\",\n",
       " 'want',\n",
       " 'it.',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'was',\n",
       " 'that',\n",
       " 'until',\n",
       " 'I',\n",
       " 'woke',\n",
       " 'up',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'night',\n",
       " 'for',\n",
       " 'a',\n",
       " 'bathroom',\n",
       " 'break',\n",
       " 'and',\n",
       " 'noticed',\n",
       " 'both',\n",
       " 'our',\n",
       " 'clothes',\n",
       " 'were',\n",
       " 'off',\n",
       " 'and',\n",
       " 'she',\n",
       " 'had',\n",
       " 'her',\n",
       " 'hand',\n",
       " 'on',\n",
       " 'my',\n",
       " 'privates.',\n",
       " 'This',\n",
       " 'obviously',\n",
       " 'freaked',\n",
       " 'me',\n",
       " 'out',\n",
       " 'and',\n",
       " 'i',\n",
       " 'kept',\n",
       " 'questioning',\n",
       " 'her',\n",
       " 'why',\n",
       " 'is',\n",
       " 'her',\n",
       " 'hand',\n",
       " 'there,',\n",
       " 'what',\n",
       " 'is',\n",
       " 'she',\n",
       " 'doing',\n",
       " '&amp;',\n",
       " 'why',\n",
       " 'are',\n",
       " 'my',\n",
       " 'clothes',\n",
       " 'off',\n",
       " '(i',\n",
       " 'slept',\n",
       " 'fully',\n",
       " 'clothed).',\n",
       " 'She',\n",
       " 'just',\n",
       " 'kept..',\n",
       " 'ignoring',\n",
       " 'me.',\n",
       " 'Asking',\n",
       " 'me',\n",
       " 'things',\n",
       " 'like,',\n",
       " '\"why',\n",
       " \"aren't\",\n",
       " 'you',\n",
       " 'excited/hard\"',\n",
       " 'to',\n",
       " '\"let\\'s',\n",
       " 'do',\n",
       " 'it,',\n",
       " \"we're\",\n",
       " 'old',\n",
       " 'enough',\n",
       " 'and',\n",
       " 'long',\n",
       " 'into',\n",
       " 'this',\n",
       " 'relationship\".',\n",
       " 'After',\n",
       " 'I',\n",
       " 'kept',\n",
       " 'refusing',\n",
       " 'i',\n",
       " 'just',\n",
       " 'got',\n",
       " 'up',\n",
       " 'and',\n",
       " 'slept',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'in',\n",
       " 'her',\n",
       " 'living',\n",
       " 'room',\n",
       " 'and',\n",
       " 'left',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'sunrise.',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'speak',\n",
       " 'because',\n",
       " 'it',\n",
       " 'felt',\n",
       " 'so',\n",
       " 'embarrassing',\n",
       " 'that',\n",
       " 'she',\n",
       " 'even',\n",
       " 'saw',\n",
       " 'me',\n",
       " 'naked',\n",
       " '(again,',\n",
       " 'very',\n",
       " 'insecure).',\n",
       " 'After',\n",
       " 'I',\n",
       " 'left',\n",
       " 'she',\n",
       " 'messaged',\n",
       " 'me',\n",
       " 'asking',\n",
       " 'why',\n",
       " 'i',\n",
       " 'acted',\n",
       " 'out',\n",
       " 'the',\n",
       " 'way',\n",
       " 'I',\n",
       " 'did.',\n",
       " 'I',\n",
       " 'replied',\n",
       " 'with',\n",
       " 'what',\n",
       " 'I',\n",
       " 'mentioned',\n",
       " 'earlier,',\n",
       " 'about',\n",
       " 'not',\n",
       " 'wanting',\n",
       " 'it',\n",
       " '&amp;',\n",
       " 'being',\n",
       " 'embarrassed',\n",
       " 'by',\n",
       " 'it.',\n",
       " 'She',\n",
       " 'told',\n",
       " 'me',\n",
       " \"it's\",\n",
       " 'normal',\n",
       " '&amp;',\n",
       " 'all',\n",
       " 'relationships',\n",
       " 'do',\n",
       " 'it..',\n",
       " 'and',\n",
       " 'because',\n",
       " \"I've\",\n",
       " 'never',\n",
       " 'been',\n",
       " 'in',\n",
       " 'one',\n",
       " 'before,',\n",
       " 'I',\n",
       " \"wouldn't\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'couples',\n",
       " 'do(?).',\n",
       " 'I',\n",
       " 'stupidly',\n",
       " 'believed',\n",
       " 'her',\n",
       " 'and',\n",
       " 'apologized',\n",
       " 'alot',\n",
       " '.',\n",
       " 'And',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'alot',\n",
       " 'like',\n",
       " 'by',\n",
       " 'buying',\n",
       " 'gifts',\n",
       " 'and',\n",
       " 'such.',\n",
       " 'Ever',\n",
       " 'since',\n",
       " 'that,',\n",
       " 'she',\n",
       " 'never',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'again..',\n",
       " 'I',\n",
       " 'think.',\n",
       " \"I'm\",\n",
       " 'not',\n",
       " 'sure',\n",
       " 'as',\n",
       " \"I'm\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'heavy',\n",
       " 'sleeper',\n",
       " 'due',\n",
       " 'to',\n",
       " 'some',\n",
       " 'health',\n",
       " 'complications',\n",
       " '(I',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'about',\n",
       " 'at',\n",
       " 'that',\n",
       " 'age).',\n",
       " 'Around',\n",
       " 'age',\n",
       " '18,',\n",
       " 'We',\n",
       " 'broke',\n",
       " 'up.',\n",
       " 'Because',\n",
       " 'she',\n",
       " 'cheated',\n",
       " 'on',\n",
       " 'me,',\n",
       " 'It',\n",
       " 'was',\n",
       " 'heartbreaking',\n",
       " 'and',\n",
       " 'painful',\n",
       " 'for',\n",
       " 'me',\n",
       " 'because',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'long-term',\n",
       " 'up',\n",
       " 'until',\n",
       " 'marriage.',\n",
       " '(I',\n",
       " 'know,',\n",
       " 'naive',\n",
       " 'thinking).',\n",
       " 'It',\n",
       " 'was',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'get',\n",
       " 'over,',\n",
       " 'but',\n",
       " 'eventually',\n",
       " 'I',\n",
       " 'did.',\n",
       " '––',\n",
       " 'At',\n",
       " 'age',\n",
       " '22',\n",
       " 'I',\n",
       " 'met',\n",
       " 'my',\n",
       " 'current',\n",
       " 'girlfriend',\n",
       " 'who',\n",
       " \"I've\",\n",
       " 'known',\n",
       " 'for',\n",
       " 'a',\n",
       " 'few',\n",
       " 'years.',\n",
       " \"I've\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'very',\n",
       " 'low',\n",
       " 'profile',\n",
       " 'private',\n",
       " 'person',\n",
       " 'since',\n",
       " 'I',\n",
       " 'broke',\n",
       " 'up',\n",
       " 'with',\n",
       " 'my',\n",
       " 'ex',\n",
       " 'at',\n",
       " '18',\n",
       " 'and',\n",
       " 'closed',\n",
       " 'myself',\n",
       " 'off',\n",
       " 'emotionally.',\n",
       " 'However,',\n",
       " 'since',\n",
       " 'being',\n",
       " 'with',\n",
       " 'my',\n",
       " 'now-girlfriend',\n",
       " '(we',\n",
       " 'recently',\n",
       " 'had',\n",
       " 'our',\n",
       " 'first',\n",
       " 'anniversary,',\n",
       " 'yay)',\n",
       " \"I've\",\n",
       " 'been',\n",
       " 'able',\n",
       " 'to',\n",
       " 'emotionally',\n",
       " 'open',\n",
       " 'up',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'more.',\n",
       " \"She's\",\n",
       " 'shared',\n",
       " 'alot',\n",
       " 'about',\n",
       " 'her',\n",
       " 'life',\n",
       " 'to',\n",
       " 'me..',\n",
       " 'both',\n",
       " 'ups',\n",
       " 'and',\n",
       " 'downs',\n",
       " '&amp;',\n",
       " 'even',\n",
       " 'the',\n",
       " 'serious',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'abuse',\n",
       " 'she',\n",
       " 'dealt',\n",
       " 'with',\n",
       " 'and',\n",
       " 'all.',\n",
       " \"I've\",\n",
       " 'recently',\n",
       " 'come',\n",
       " 'to',\n",
       " 'an',\n",
       " 'understanding',\n",
       " '(better',\n",
       " 'late',\n",
       " 'than',\n",
       " 'never..)',\n",
       " 'that',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'me',\n",
       " 'when',\n",
       " 'I',\n",
       " 'was',\n",
       " '15',\n",
       " 'with',\n",
       " 'my',\n",
       " 'ex',\n",
       " 'was',\n",
       " 'NOT',\n",
       " 'normal',\n",
       " 'and',\n",
       " 'was',\n",
       " 'actually',\n",
       " 'sexual',\n",
       " 'abuse/assault.',\n",
       " 'Since',\n",
       " 'then',\n",
       " \"I've\",\n",
       " 'been',\n",
       " 'spiralling',\n",
       " 'down',\n",
       " 'because',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'so',\n",
       " 'disgusted',\n",
       " 'by',\n",
       " 'it.',\n",
       " 'To',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'victim',\n",
       " 'just',\n",
       " 'idk..',\n",
       " 'hurt',\n",
       " 'me?',\n",
       " 'I',\n",
       " \"couldn't\",\n",
       " 'stop',\n",
       " 'thinking',\n",
       " 'about',\n",
       " 'it',\n",
       " 'to',\n",
       " 'the',\n",
       " 'point',\n",
       " 'I',\n",
       " 'just',\n",
       " 'shutdown.',\n",
       " \"It's\",\n",
       " 'been',\n",
       " 'on',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'for',\n",
       " 'weeks',\n",
       " 'now.',\n",
       " 'My',\n",
       " 'girlfriend',\n",
       " 'has',\n",
       " 'noticed',\n",
       " 'my',\n",
       " 'rapid',\n",
       " 'mood',\n",
       " 'shift',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'worried',\n",
       " 'about',\n",
       " 'me',\n",
       " 'because',\n",
       " 'I',\n",
       " \"haven't\",\n",
       " 'told',\n",
       " 'her',\n",
       " 'why.',\n",
       " \"I'm\",\n",
       " 'looking',\n",
       " 'for',\n",
       " 'just',\n",
       " 'advice',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'her',\n",
       " 'about',\n",
       " 'it?',\n",
       " 'My',\n",
       " 'biggest',\n",
       " 'fear',\n",
       " 'is',\n",
       " 'that',\n",
       " \"she'll\",\n",
       " 'think',\n",
       " 'of',\n",
       " 'me',\n",
       " 'as',\n",
       " 'less',\n",
       " 'of',\n",
       " 'a',\n",
       " 'man',\n",
       " 'because',\n",
       " 'I',\n",
       " 'let',\n",
       " 'it',\n",
       " 'happen..',\n",
       " \"I'm\",\n",
       " 'just',\n",
       " 'a',\n",
       " 'mess.',\n",
       " 'Hopefully',\n",
       " 'this',\n",
       " 'post',\n",
       " 'somewhat',\n",
       " 'made',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'someone.',\n",
       " 'I',\n",
       " 'appreciate',\n",
       " 'any',\n",
       " 'talks',\n",
       " 'of',\n",
       " 'advice',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'approach',\n",
       " 'this',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'her.',\n",
       " 'Thank',\n",
       " 'you.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpost.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
