{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import FreqDist, pos_tag\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./data/json_pr.json') as x:\n",
    "    prj = json.load(x)\n",
    "    \n",
    "with open ('./data/json_ra.json') as y:\n",
    "    raj = json.load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj = pd.DataFrame(prj)\n",
    "df_raj = pd.DataFrame(raj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #confirm latest dfs: 0 nulls.  confirmed\n",
    "# print(df_prj.isnull().sum())\n",
    "# print(df_raj.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All posts left in original format, no special character elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure analysis separate from diction:\n",
    "\n",
    "Number of sentences, sentence length, and *rate* of punctuation use (. , ?) could be informative.  How preserve?  \n",
    "\n",
    "2 separate analyses: nlp based on Bag of Words (0 structure or punctuation retention), and one based on structure.  \n",
    "\n",
    "Word Frequencies (see lesson 5.04) counts *everything* - punctuation, numbers, etc.  But I'd rather use WordFreq, CountVec, etc for lexicon and reduce punctuation down to counts.\n",
    "\n",
    "New columns for total wordcount, # sentences, average sentence length, total commas / ? / ! (which can be changed into per wordcount or per sentence 'rates' later).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with sentence count\n",
    "Methods based on experimentation in scratch notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj['num_sentences'] = df_prj['selftext'].map(lambda x: len(re.split(r'[.!?]+', x))-1)\n",
    "df_raj['num_sentences'] = df_raj['selftext'].map(lambda x: len(re.split(r'[.!?]+', x))-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with average sentence length (by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len counts spaces and characters.  you want wordcount.\n",
    "length = 'this is twenty lettersS..!?,'\n",
    "len(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raj_mini['words_per_sentence'] = \n",
    "\n",
    "def avg_words_per_sentence(df):\n",
    "    \n",
    "    all_sentences_bypost = []\n",
    "\n",
    "    for i in df['selftext']:\n",
    "        all_sentences_bypost.append(re.split(r'[.!?]+', i))\n",
    "    \n",
    "    avgwordsper = []\n",
    "\n",
    "    for post in all_sentences_bypost:\n",
    "        wordspersentence = []\n",
    "        for i in post:\n",
    "            wordspersentence.append(len(i.split()))\n",
    "        avgwordsper.append(int(np.mean(wordspersentence).round()))\n",
    "        \n",
    "    return avgwordsper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['avg_words_per_sent'] = avg_words_per_sentence(df_raj)\n",
    "df_prj['avg_words_per_sent'] = avg_words_per_sentence(df_prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns with total wordcount, commas, ?s, and !s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['wordcount'] = df_raj['selftext'].map(lambda x: len(x.split()))\n",
    "df_prj['wordcount'] = df_prj['selftext'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['comma_count'] = df_raj['selftext'].map(lambda x: x.count(','))\n",
    "df_raj['qmark_count'] = df_raj['selftext'].map(lambda x: x.count('?'))\n",
    "df_raj['exclamatios'] = df_raj['selftext'].map(lambda x: x.count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punc(punc, df):\n",
    "    perpost = []\n",
    "    for i in df['selftext']:\n",
    "        perpost.append(i.count(punc))\n",
    "    return perpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj['comma_count'] = count_punc(',', df_prj)\n",
    "df_prj['qmark_count'] = count_punc('?', df_prj)\n",
    "df_prj['exclamatios'] = count_punc('!', df_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No. Use total comma count, ! count, ? count\n",
    "## Can turn into 'rate' later.  Prob by sentence count, not wordcount...\n",
    "\n",
    "# def punc_rate(punctuation, df):\n",
    "    \n",
    "#     puncs=[]\n",
    "#     punc_rate=[]\n",
    "    \n",
    "#     for i in df['selftext']:\n",
    "#         commas.append(len(re.findall(punctuation, i)))\n",
    "    \n",
    "#     for j in commas:\n",
    "#         comma_rate.append(commas[j]/df['wordcount'][j])\n",
    "        \n",
    "#     return punc_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raj['comma_rate'] = punc_rate(',', df_raj)\n",
    "# df_prj['comma_rate'] = punc_rate(',', df_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overly complicated just use map + str.count\n",
    "# run into issues with 0s with re.findall I think\n",
    "\n",
    "# def punc_count(punc, df):\n",
    "#     count = []\n",
    "#     for i in df['selftext']:\n",
    "#         count.append(len(re.findall(punc, i)))\n",
    "#     return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column is_mobile to distinguish if posted by mobile user\n",
    "Users generally announce '(on) Mobile\" if they are posting from mobile phones, but there is no consistent format to this announcement.  Sometimes it's a sentence, sometimes a single word declaration.  ID'ing is_mobile by simply searching for the word 'mobile' will also flag posts containing words like 'automobile', 'immobile', 'mobile bank account', etc ('shitmobile' is one I found that I did not expect...).  \n",
    "Spot check 100 posts from each df to get rate of false positives to see if an is_mobile column is informative.\n",
    "\n",
    "---\n",
    "\n",
    "Results from check: \n",
    "* relationship_advice had 10.6% false positives (10 of total 94)\n",
    "* pettyrev had 8% false positive rate (8 of 100 random checks, total count 395)  \n",
    "\n",
    "##### Assume a fp rate of 9%, still worth identifying.  Pretty consistent fp rate across both dataframes makes it a reliable enough metric.  Concatenate before adding column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ctrl-f in jq pretty-print to check if posting from mobile, or talking about mobile homes, etc. \n",
    "df_raj.loc[df_raj['selftext'].str.lower().str.contains('mobile')].to_json('./scratch/is_mobile_ra.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10/94 records containing 'mobile' NOT posted from mobile user \n",
    "#is_mobile = false positive rate 10.6% for RA\n",
    "df_raj.loc[df_raj['selftext'].str.lower().str.contains('mobile')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for pettyrev.  len = 395.  check first 100.\n",
    "df_prj.loc[df_prj['selftext'].str.lower().str.contains('mobile')].head(100).to_json('./scratch/is_mobile_pr.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_structured = pd.concat([df_prj, df_raj], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_structured['is_mobile'] = combo_structured['selftext'].apply(lambda x: 1 if 'mobile' in x.lower() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11519\n",
       "1      489\n",
       "Name: is_mobile, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_structured.is_mobile.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csv and json versions\n",
    "combo_structured.to_csv('./data/allposts_struc.csv', index=False)\n",
    "combo_structured.to_json('./data/j_allposts_struc.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special characters to scrub:\n",
    "* **all URLs:  start with https://...** \n",
    "* characters with alphanum attached: I think remove these whole units first (or the attached text), otherwise left with 'n', 'amp', 'u username', etc that won't be picked up by subsequent alphanum filter?\n",
    "    * ' & amp ;\n",
    "    * \\n\n",
    "    * u/username\n",
    "    * (Mobile)\n",
    "        * edit: leave mobile in place, could be differentiating feature between subreddits\n",
    "        * edit 2: no matter if scrub or not, already have column indicating is_mobile\n",
    "* all non-alpha-numerics: \\, /, &, * , \"\", '', -\n",
    "* emojis \n",
    "* I don't think numbers will be informative\n",
    "* stopwords='english' : word freq table for petty revenge showed top words were all stopwords.  Not informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagowords = combo_structured.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with urls so you don't accidentally turn them into text by ditching identifying characters\n",
    "bagowords['selftext'] = bagowords['selftext'].apply(lambda x: re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenizer drops many special characters but keeps 'amp' from &amp;\n",
    "# drop that unit specifically first:\n",
    "bagowords['selftext'] = bagowords['selftext'].apply(lambda x: x.replace('&amp;',''))\n",
    "#also apostrophes: just merge into one word (don't -> dont)\n",
    "bagowords['selftext'] = bagowords['selftext'].apply(lambda x: x.replace(\"'\",''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'couldve wouldve dont Im isnt'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"could've would've don't I'm isn't\"\n",
    "test.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize then return only alpha characters re-joined in original order\n",
    "#NUMBERS removed\n",
    "#special characters removed\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagowords['selftext'] = bagowords['selftext'].apply(lambda x: ' '.join([i for i in tokenizer.tokenize(x.lower()) if i.isascii() and i.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 1s, sys: 2min 58s, total: 26min\n",
      "Wall time: 2h 5min 55s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# #remove stopwords\n",
    "# bagowords['selftext'] = bagowords['selftext'].apply(lambda x: [i for i in x if i not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results in jq pretty print\n",
    "bagowords.to_json('./scratch/check_selftext_b.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word frequency function from lesson 5.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_table(text, count=None):\n",
    "\n",
    "    freqs = FreqDist(text).most_common()\n",
    "    prob = [round(x[1]/len(text),4) for x in freqs]\n",
    "\n",
    "    freq = zip(freqs,prob)\n",
    "\n",
    "    comp_freqs = []\n",
    "    for s,p in freq:\n",
    "        comp_freqs.append([s[0],s[1],p])\n",
    "\n",
    "    comp_freqs.sort(key=lambda tup: tup[1], reverse=True) #Sort the list so it's in order, big to small\n",
    "\n",
    "    if count == None:\n",
    "        most = comp_freqs[:26]    \n",
    "        hapax = comp_freqs[:-25:-1]\n",
    "    else:\n",
    "        most = comp_freqs[:count + 1]\n",
    "        hapax = comp_freqs[:-count:-1]\n",
    "\n",
    "    print('Most Common \\t\\t  Least Common')\n",
    "\n",
    "    for i in zip(most, hapax):\n",
    "        print(i[0], \" \"*(24-len(str(i[0]))),i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettyrev_text = ' '.join(list(bagowords['selftext'])).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4385515"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pettyrev_text) #4.3M! cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common \t\t  Least Common\n",
      "['i', 187490, 0.0428]     ['downgraded', 1, 0.0]\n",
      "['and', 149921, 0.0342]   ['supine', 1, 0.0]\n",
      "['the', 140221, 0.032]    ['symptomatic', 1, 0.0]\n",
      "['to', 138681, 0.0316]    ['agh', 1, 0.0]\n",
      "['a', 97909, 0.0223]      ['untalkable', 1, 0.0]\n",
      "['my', 64050, 0.0146]     ['loong', 1, 0.0]\n",
      "['of', 60956, 0.0139]     ['rifts', 1, 0.0]\n",
      "['me', 57438, 0.0131]     ['sextng', 1, 0.0]\n",
      "['it', 55755, 0.0127]     ['sexuakky', 1, 0.0]\n",
      "['was', 55636, 0.0127]    ['turban', 1, 0.0]\n",
      "['that', 54384, 0.0124]   ['overweighted', 1, 0.0]\n",
      "['in', 53684, 0.0122]     ['wizard', 1, 0.0]\n",
      "['he', 52457, 0.012]      ['corollary', 1, 0.0]\n",
      "['she', 51416, 0.0117]    ['signup', 1, 0.0]\n",
      "['her', 47235, 0.0108]    ['hotliners', 1, 0.0]\n",
      "['for', 42346, 0.0097]    ['repetitions', 1, 0.0]\n",
      "['with', 37952, 0.0087]   ['nevers', 1, 0.0]\n",
      "['but', 36105, 0.0082]    ['yeaah', 1, 0.0]\n",
      "['we', 34754, 0.0079]     ['impetuousness', 1, 0.0]\n",
      "['this', 34482, 0.0079]   ['rehabbing', 1, 0.0]\n",
      "['so', 34446, 0.0079]     ['subleased', 1, 0.0]\n",
      "['on', 32267, 0.0074]     ['keylogger', 1, 0.0]\n",
      "['is', 30921, 0.0071]     ['fairest', 1, 0.0]\n",
      "['him', 26071, 0.0059]    ['gilmore', 1, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#all single letters even though apostrophes removed...\n",
    "freq_table(pettyrev_text, count=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_self</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_words_per_sent</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>qmark_count</th>\n",
       "      <th>exclamatios</th>\n",
       "      <th>is_mobile</th>\n",
       "      <th>selftext_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [selftext, subreddit, created_utc, is_self, score, title, author, num_comments, timestamp, year, num_sentences, avg_words_per_sent, wordcount, comma_count, qmark_count, exclamatios, is_mobile, selftext_b]\n",
       "Index: []"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagowords.loc[bagowords['selftext'].str.contains('配')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my friend and i met in high school when he was a foreign exchange student he barely spoke english and i didn t even know what japanese was but we connected like crazy it was years ago but we ve still kept in contact my major in college was japanese and i lived there for a short while we ve always kept in contact and we speak somewhat regularly i ve always considered him my best friend but i sent this to him two days ago psa he knew my marriage was on the fritz もう長い間 腹を割って話してないね 僕たちは分かれてから別の場所で それぞれの道のりを歩んできた 君も僕も 大変で長い道だったと思う 僕は君を本当の兄のように思って 慕っている どんなに離れて暮らしても それは変わることはない 知り合った日から つながりはずっとここにあるから それで 僕は最近離婚のことに緊張して すごく辛い 誰かに話そうと思っても そういう人がいないんだ 君にも電話しようと思ったんだけど 余計な心配をかけちゃいけないと思って 結局やめちゃった 僕はただ 友達が必要なんだ 廣敬と話したいよ loosely translated i know we haven t spoken to each other meaningfully in a long time we have both been going through so much lately in different ways but each way hard in our own respects i love you like a brother and that s never going to go away no matter how far we live from each other we connected in our youth and the connection will always be there my divorce is killing me i have nobody close to talk to i pick up the phone to call you but i think it would be too big of a burden for you so i don t press send i just need my friend i miss you i saw that he read it two days ago and there s been no reply a i ll talk to you later or a wow can t read that right now kind of response would have been ok but to totally not respond when i can see that he read the message what should i think'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagowords['selftext_b'][6071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'wherever', 'two', 'we', 'no', 'before', 'couldnt', 'show', 'go', 'enough', 'former', 'eg', 'whom', 'those', 'very', 'move', 'its', 'anyone', 'others', 'interest', 'the', 'many', 'am', 'give', 'your', 'name', 'into', 'whose', 'otherwise', 'put', 'nor', 'she', 'as', 'how', 'full', 'almost', 'keep', 'for', 'twelve', 'forty', 'someone', 'less', 'whereas', 'detail', 'system', 'against', 'than', 'been', 'somewhere', 'moreover', 'and', 'after', 'next', 'more', 'every', 'five', 'herein', 'hundred', 'made', 'first', 'much', 'whole', 'only', 'should', 'sometimes', 'thin', 'same', 'you', 'thence', 'his', 'yours', 'meanwhile', 'thereupon', 'their', 'while', 'on', 'due', 'ten', 'rather', 'themselves', 'a', 'ever', 'ours', 'down', 'ie', 'off', 'fill', 'why', 'few', 'formerly', 'see', 'would', 'may', 'has', 'nobody', 'yourself', 'please', 'where', 'in', 'so', 'beyond', 'when', 'bill', 'over', 'whoever', 'mine', 'ltd', 'to', 'find', 'cry', 'else', 'third', 'namely', 'this', 'our', 'thru', 'seeming', 'well', 'were', 'side', 'below', 'are', 'front', 'hereby', 'onto', 'will', 'sixty', 'hers', 'towards', 'but', 'was', 'eleven', 'afterwards', 'ourselves', 'seem', 'whereafter', 'since', 'myself', 'fifteen', 'what', 'itself', 'thereafter', 'have', 'twenty', 'toward', 'sometime', 'might', 'other', 'hereafter', 'here', 'fire', 'beside', 'therefore', 'until', 'seemed', 'thus', 'further', 'latterly', 'already', 'top', 'of', 'any', 'by', 'without', 'nevertheless', 'un', 'they', 'alone', 'amoungst', 'noone', 'except', 'me', 'hasnt', 'do', 'con', 'i', 'several', 'four', 'everyone', 'always', 'through', 'he', 'could', 'often', 'another', 'along', 'whenever', 'had', 'above', 'thick', 'something', 'either', 'nine', 'all', 'anyhow', 'per', 'these', 'beforehand', 'her', 'cant', 'indeed', 'there', 'also', 'behind', 'none', 'at', 'or', 'still', 'yet', 'amongst', 'etc', 'that', 'together', 'such', 'is', 'became', 'everywhere', 'perhaps', 'found', 'who', 'eight', 'never', 'upon', 'therein', 'about', 'whereupon', 'six', 'anyway', 'most', 'serious', 'whence', 'around', 'even', 'again', 'becomes', 'neither', 'wherein', 'both', 'be', 'yourselves', 'once', 'latter', 'one', 'them', 'anything', 'empty', 'can', 'thereby', 'from', 'nowhere', 'under', 'via', 'inc', 'co', 'cannot', 'hence', 'anywhere', 'back', 'whither', 'seems', 'out', 'being', 'fifty', 'although', 'because', 'last', 'him', 'mill', 'mostly', 'if', 'nothing', 'call', 'sincere', 'which', 'part', 'describe', 'within', 'then', 'with', 'too', 'take', 'however', 'become', 'own', 'my', 'whatever', 'de', 'himself', 'becoming', 'an', 'though', 'throughout', 'some', 'whether', 'herself', 'it', 'amount', 'hereupon', 'not', 'whereby', 'least', 'between', 'across', 'everything', 'done', 'each', 'now', 'must', 're', 'elsewhere', 'somehow', 'get', 'three', 'up', 'during', 'besides', 'us', 'bottom', 'among'})\n"
     ]
    }
   ],
   "source": [
    "print(CountVectorizer(stop_words='english').get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-ebc6103db022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cop' is not defined"
     ]
    }
   ],
   "source": [
    "print(cop.isascii() and cop.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
