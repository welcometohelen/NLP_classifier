{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import FreqDist, pos_tag\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./data/json_pr.json') as x:\n",
    "    prj = json.load(x)\n",
    "    \n",
    "with open ('./data/json_ra.json') as y:\n",
    "    raj = json.load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj = pd.DataFrame(prj)\n",
    "df_raj = pd.DataFrame(raj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #confirm latest dfs: 0 nulls.  confirmed\n",
    "# print(df_prj.isnull().sum())\n",
    "# print(df_raj.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All posts left in original format, no special character elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure analysis separate from diction:\n",
    "\n",
    "Number of sentences, sentence length, and *rate* of punctuation use (. , ?) could be informative.  How preserve?  \n",
    "\n",
    "2 separate analyses: nlp based on Bag of Words (0 structure or punctuation retention), and one based on structure.  \n",
    "\n",
    "Word Frequencies (see lesson 5.04) counts *everything* - punctuation, numbers, etc.  But I'd rather use WordFreq, CountVec, etc for lexicon and reduce punctuation down to counts.\n",
    "\n",
    "New columns for total wordcount, # sentences, average sentence length, total commas / ? / ! (which can be changed into per wordcount or per sentence 'rates' later).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with sentence count\n",
    "Methods based on experimentation in scratch notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj['num_sentences'] = df_prj['selftext'].map(lambda x: len(re.split(r'[.!?]+', x))-1)\n",
    "df_raj['num_sentences'] = df_raj['selftext'].map(lambda x: len(re.split(r'[.!?]+', x))-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column with average sentence length (by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len counts spaces and characters.  you want wordcount.\n",
    "length = 'this is twenty lettersS..!?,'\n",
    "len(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raj_mini['words_per_sentence'] = \n",
    "\n",
    "def avg_words_per_sentence(df):\n",
    "    \n",
    "    all_sentences_bypost = []\n",
    "\n",
    "    for i in df['selftext']:\n",
    "        all_sentences_bypost.append(re.split(r'[.!?]+', i))\n",
    "    \n",
    "    avgwordsper = []\n",
    "\n",
    "    for post in all_sentences_bypost:\n",
    "        wordspersentence = []\n",
    "        for i in post:\n",
    "            wordspersentence.append(len(i.split()))\n",
    "        avgwordsper.append(int(np.mean(wordspersentence).round()))\n",
    "        \n",
    "    return avgwordsper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['avg_words_per_sent'] = avg_words_per_sentence(df_raj)\n",
    "df_prj['avg_words_per_sent'] = avg_words_per_sentence(df_prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns with total wordcount, commas, ?s, and !s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['wordcount'] = df_raj['selftext'].map(lambda x: len(x.split()))\n",
    "df_prj['wordcount'] = df_prj['selftext'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raj['comma_count'] = df_raj['selftext'].map(lambda x: x.count(','))\n",
    "df_raj['qmark_count'] = df_raj['selftext'].map(lambda x: x.count('?'))\n",
    "df_raj['exclamatios'] = df_raj['selftext'].map(lambda x: x.count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punc(punc, df):\n",
    "    perpost = []\n",
    "    for i in df['selftext']:\n",
    "        perpost.append(i.count(punc))\n",
    "    return perpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prj['comma_count'] = count_punc(',', df_prj)\n",
    "df_prj['qmark_count'] = count_punc('?', df_prj)\n",
    "df_prj['exclamatios'] = count_punc('!', df_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No. Use total comma count, ! count, ? count\n",
    "## Can turn into 'rate' later.  Prob by sentence count, not wordcount...\n",
    "\n",
    "# def punc_rate(punctuation, df):\n",
    "    \n",
    "#     puncs=[]\n",
    "#     punc_rate=[]\n",
    "    \n",
    "#     for i in df['selftext']:\n",
    "#         commas.append(len(re.findall(punctuation, i)))\n",
    "    \n",
    "#     for j in commas:\n",
    "#         comma_rate.append(commas[j]/df['wordcount'][j])\n",
    "        \n",
    "#     return punc_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raj['comma_rate'] = punc_rate(',', df_raj)\n",
    "# df_prj['comma_rate'] = punc_rate(',', df_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overly complicated just use map + str.count\n",
    "# run into issues with 0s with re.findall I think\n",
    "\n",
    "# def punc_count(punc, df):\n",
    "#     count = []\n",
    "#     for i in df['selftext']:\n",
    "#         count.append(len(re.findall(punc, i)))\n",
    "#     return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_self</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_words_per_sent</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>qmark_count</th>\n",
       "      <th>exclamatios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI remember during my first year of colleg...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1617115680</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>How the tables have turned.</td>\n",
       "      <td>BestGuessGuest</td>\n",
       "      <td>79</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>227</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A bit long TDRL at the end. First post on mobi...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1617119446</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>I know how to use the radio</td>\n",
       "      <td>Negative_Shake1478</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>391</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior year of high school. Last school dance ...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1617138131</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Made a girl cry after she stuck gum in my hair</td>\n",
       "      <td>The_Bee_Sneeze</td>\n",
       "      <td>44</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>410</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When I was working as a barista, there were tw...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1617168650</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Lady waits to use the bathroom every night at ...</td>\n",
       "      <td>snailtearstains</td>\n",
       "      <td>557</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>517</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First let me say: I fucking hate it when peopl...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1617204366</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Rude lady talking on phone</td>\n",
       "      <td>Xxtratourettestriall</td>\n",
       "      <td>20</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>233</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>I hate cleaning the shower.  I think my wife g...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1468803783</td>\n",
       "      <td>True</td>\n",
       "      <td>45</td>\n",
       "      <td>Cleaning the shower</td>\n",
       "      <td>d0ntblink</td>\n",
       "      <td>12</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>Recently, I attended a sewing summer camp. Pre...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1468804377</td>\n",
       "      <td>True</td>\n",
       "      <td>2534</td>\n",
       "      <td>Save 20 seconds, lose 5 minutes, b----.</td>\n",
       "      <td>Falcon10301</td>\n",
       "      <td>148</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>522</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>Was at a show with my wife recently.  We were ...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1468812440</td>\n",
       "      <td>True</td>\n",
       "      <td>154</td>\n",
       "      <td>Be rude and pushy at a crowded show, enjoy a w...</td>\n",
       "      <td>Doodieboi7</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>134</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911</th>\n",
       "      <td>So one night, a few months ago, a friend of mi...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1468827535</td>\n",
       "      <td>True</td>\n",
       "      <td>109</td>\n",
       "      <td>Yeah, I got a tip for you....</td>\n",
       "      <td>AceBobcat</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>174</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>The Project Manager on my work site is a greed...</td>\n",
       "      <td>pettyrevenge</td>\n",
       "      <td>1468832080</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "      <td>Payback's a phlegmy bitch!</td>\n",
       "      <td>josmille</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5913 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               selftext     subreddit  \\\n",
       "0      \\n\\nI remember during my first year of colleg...  pettyrevenge   \n",
       "1     A bit long TDRL at the end. First post on mobi...  pettyrevenge   \n",
       "2     Senior year of high school. Last school dance ...  pettyrevenge   \n",
       "3     When I was working as a barista, there were tw...  pettyrevenge   \n",
       "4     First let me say: I fucking hate it when peopl...  pettyrevenge   \n",
       "...                                                 ...           ...   \n",
       "5908  I hate cleaning the shower.  I think my wife g...  pettyrevenge   \n",
       "5909  Recently, I attended a sewing summer camp. Pre...  pettyrevenge   \n",
       "5910  Was at a show with my wife recently.  We were ...  pettyrevenge   \n",
       "5911  So one night, a few months ago, a friend of mi...  pettyrevenge   \n",
       "5912  The Project Manager on my work site is a greed...  pettyrevenge   \n",
       "\n",
       "      created_utc  is_self  score  \\\n",
       "0      1617115680     True      1   \n",
       "1      1617119446     True      1   \n",
       "2      1617138131     True      1   \n",
       "3      1617168650     True      1   \n",
       "4      1617204366     True      1   \n",
       "...           ...      ...    ...   \n",
       "5908   1468803783     True     45   \n",
       "5909   1468804377     True   2534   \n",
       "5910   1468812440     True    154   \n",
       "5911   1468827535     True    109   \n",
       "5912   1468832080     True     49   \n",
       "\n",
       "                                                  title                author  \\\n",
       "0                           How the tables have turned.        BestGuessGuest   \n",
       "1                           I know how to use the radio    Negative_Shake1478   \n",
       "2        Made a girl cry after she stuck gum in my hair        The_Bee_Sneeze   \n",
       "3     Lady waits to use the bathroom every night at ...       snailtearstains   \n",
       "4                            Rude lady talking on phone  Xxtratourettestriall   \n",
       "...                                                 ...                   ...   \n",
       "5908                                Cleaning the shower             d0ntblink   \n",
       "5909            Save 20 seconds, lose 5 minutes, b----.           Falcon10301   \n",
       "5910  Be rude and pushy at a crowded show, enjoy a w...            Doodieboi7   \n",
       "5911                      Yeah, I got a tip for you....             AceBobcat   \n",
       "5912                         Payback's a phlegmy bitch!              josmille   \n",
       "\n",
       "      num_comments   timestamp  year  num_sentences  avg_words_per_sent  \\\n",
       "0               79  2021-03-30  2021             12                  17   \n",
       "1               23  2021-03-30  2021             25                  15   \n",
       "2               44  2021-03-30  2021             39                  10   \n",
       "3              557  2021-03-30  2021             44                  12   \n",
       "4               20  2021-03-31  2021              8                  26   \n",
       "...            ...         ...   ...            ...                 ...   \n",
       "5908            12  2016-07-17  2016              3                  16   \n",
       "5909           148  2016-07-17  2016             29                  18   \n",
       "5910             8  2016-07-17  2016              8                  15   \n",
       "5911            10  2016-07-17  2016             12                  14   \n",
       "5912            11  2016-07-17  2016              3                  24   \n",
       "\n",
       "      wordcount  comma_count  qmark_count  exclamatios  \n",
       "0           227            6            0            0  \n",
       "1           391            9            0            0  \n",
       "2           410           27            2            0  \n",
       "3           517           18            3            2  \n",
       "4           233            4            1            2  \n",
       "...         ...          ...          ...          ...  \n",
       "5908         65            3            0            0  \n",
       "5909        522           29            3            1  \n",
       "5910        134            2            0            0  \n",
       "5911        174           15            0            0  \n",
       "5912         95            2            0            0  \n",
       "\n",
       "[5913 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column is_mobile to distinguish if posted by mobile user\n",
    "Users generally announce '(on) Mobile\" if they are posting from mobile phones, but there is no consistent format to this announcement.  Sometimes it's a sentence, sometimes a single word declaration.  ID'ing is_mobile by simply searching for the word 'mobile' will also flag posts containing words like 'automobile', 'immobile', 'mobile bank account', etc ('shitmobile' is one I found that I did not expect...).  \n",
    "Spot check 100 posts from each df to get rate of false positives to see if an is_mobile column is informative.\n",
    "\n",
    "---\n",
    "\n",
    "Results from check: \n",
    "* relationship_advice had 10.6% false positives (10 of total 94)\n",
    "* pettyrev had 8% false positive rate (8 of 100 random checks, total count 395)  \n",
    "\n",
    "##### Assume a fp rate of 9%, still worth identifying.  Pretty consistent fp rate across both dataframes makes it a reliable enough metric.  Concatenate before adding column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ctrl-f in jq pretty-print to check if posting from mobile, or talking about mobile homes, etc. \n",
    "df_raj.loc[df_raj['selftext'].str.lower().str.contains('mobile')].to_json('./scratch/is_mobile_ra.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10/94 records containing 'mobile' NOT posted from mobile user \n",
    "#is_mobile = false positive rate 10.6% for RA\n",
    "df_raj.loc[df_raj['selftext'].str.lower().str.contains('mobile')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for pettyrev.  len = 395.  check first 100.\n",
    "df_prj.loc[df_prj['selftext'].str.lower().str.contains('mobile')].head(100).to_json('./scratch/is_mobile_pr.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_structured = pd.concat([df_prj, df_raj], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_structured['is_mobile'] = combo_structured['selftext'].apply(lambda x: 1 if 'mobile' in x.lower() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11519\n",
       "1      489\n",
       "Name: is_mobile, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_structured.is_mobile.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csv and json versions\n",
    "combo_structured.to_csv('./data/allposts_struc.csv', index=False)\n",
    "combo_structured.to_json('./data/j_allposts_struc.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special characters to scrub:\n",
    "* characters with alphanum attached: I think remove these whole units first (or the attached text), otherwise left with 'n', 'amp', 'u username', etc that won't be picked up by subsequent alphanum filter\n",
    "    * ' & amp ;\n",
    "    * \\n\n",
    "    * u/username\n",
    "    * (Mobile)\n",
    "        * edit: leave mobile in place, could be differentiating feature between subreddits\n",
    "* all non-alpha-numerics: \\, /, &, * , \"\", '', -\n",
    "* emojis\n",
    "* any URLs:  start with https://...  \n",
    "* I don't think numbers will be informative\n",
    "* leave punctuation in place?  ADHD-comma anecdote, prevalence of ! ? might be informative?\n",
    "    * RA likely contains ? all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prj.loc[df_prj['selftext'].str.contains('mobile|Mobile')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raj.loc[df_raj['selftext'].str.contains('mobile|Mobile')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sorry',\n",
       " 'in',\n",
       " 'advance',\n",
       " 'if',\n",
       " 'my',\n",
       " 'texts',\n",
       " 'are',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'sloppy..',\n",
       " \"It's\",\n",
       " 'taken',\n",
       " 'me',\n",
       " 'a',\n",
       " 'long',\n",
       " 'while',\n",
       " 'to',\n",
       " 'finally',\n",
       " 'write',\n",
       " 'this',\n",
       " 'out',\n",
       " 'as',\n",
       " \"I'm\",\n",
       " 'just',\n",
       " 'very',\n",
       " 'unsure.',\n",
       " 'Side',\n",
       " 'notes:',\n",
       " \"I'm\",\n",
       " 'not',\n",
       " 'from',\n",
       " 'USA.',\n",
       " \"I'm\",\n",
       " 'from',\n",
       " 'Asia',\n",
       " '&amp;',\n",
       " 'My',\n",
       " 'country',\n",
       " 'to',\n",
       " 'hers',\n",
       " 'is',\n",
       " 'about',\n",
       " '30m',\n",
       " 'flight',\n",
       " '&amp;',\n",
       " 'cheap.',\n",
       " 'Backstory;',\n",
       " 'When',\n",
       " 'I',\n",
       " 'was',\n",
       " '14,',\n",
       " 'I',\n",
       " 'had',\n",
       " 'my',\n",
       " 'first',\n",
       " 'ever',\n",
       " \"'real'\",\n",
       " 'relationship.',\n",
       " 'At',\n",
       " 'the',\n",
       " 'time',\n",
       " 'it',\n",
       " 'was',\n",
       " 'solely',\n",
       " 'online',\n",
       " 'until',\n",
       " 'I',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'over',\n",
       " 'to',\n",
       " 'her',\n",
       " 'country',\n",
       " '(accompanied',\n",
       " 'by',\n",
       " 'my',\n",
       " \"mom's\",\n",
       " 'friend)',\n",
       " 'twice',\n",
       " 'a',\n",
       " 'month',\n",
       " 'when',\n",
       " 'I',\n",
       " 'was',\n",
       " '15.',\n",
       " 'Before',\n",
       " 'you',\n",
       " 'hammer',\n",
       " 'about',\n",
       " 'my',\n",
       " 'parents',\n",
       " 'not',\n",
       " 'seeing',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'in',\n",
       " 'our',\n",
       " 'age',\n",
       " 'gap,',\n",
       " 'she',\n",
       " 'always',\n",
       " 'made',\n",
       " 'me',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'that',\n",
       " 'we',\n",
       " 'were',\n",
       " 'only',\n",
       " 'a',\n",
       " 'year',\n",
       " 'apart',\n",
       " 'because',\n",
       " 'according',\n",
       " 'to',\n",
       " 'her,',\n",
       " '\"they',\n",
       " \"wouldn't\",\n",
       " 'understand',\n",
       " 'our',\n",
       " 'love\".',\n",
       " 'Looking',\n",
       " 'back,',\n",
       " 'I',\n",
       " 'admit',\n",
       " 'I',\n",
       " 'was',\n",
       " 'an',\n",
       " 'idiot',\n",
       " 'for',\n",
       " 'not',\n",
       " 'seeing',\n",
       " 'how',\n",
       " 'a',\n",
       " '15',\n",
       " 'year',\n",
       " 'old',\n",
       " '&amp;',\n",
       " 'a',\n",
       " '18',\n",
       " 'year',\n",
       " 'old',\n",
       " 'being',\n",
       " 'in',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'was',\n",
       " 'not',\n",
       " 'only',\n",
       " 'a',\n",
       " 'red',\n",
       " 'flag',\n",
       " 'but',\n",
       " 'also',\n",
       " 'well..',\n",
       " 'illegal.',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'my',\n",
       " 'rational',\n",
       " 'thinking',\n",
       " 'was',\n",
       " 'put',\n",
       " 'aside',\n",
       " 'because',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " '(still',\n",
       " 'am)',\n",
       " 'insecure',\n",
       " 'kid',\n",
       " 'and',\n",
       " 'to',\n",
       " 'be',\n",
       " 'approached',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'a',\n",
       " 'kind',\n",
       " 'and',\n",
       " 'good',\n",
       " 'looking',\n",
       " 'girl',\n",
       " 'was',\n",
       " 'just',\n",
       " 'amazing',\n",
       " 'for',\n",
       " 'me.',\n",
       " 'I',\n",
       " 'always',\n",
       " 'told',\n",
       " 'her',\n",
       " '(because',\n",
       " 'of',\n",
       " 'my',\n",
       " 'upbringing)',\n",
       " 'that',\n",
       " 'sex',\n",
       " 'was',\n",
       " 'completely',\n",
       " 'off',\n",
       " 'the',\n",
       " 'table.',\n",
       " 'I',\n",
       " 'just',\n",
       " \"wasn't\",\n",
       " 'into',\n",
       " 'it',\n",
       " 'after',\n",
       " 'finding',\n",
       " 'out',\n",
       " 'about',\n",
       " 'pregnancies',\n",
       " 'and',\n",
       " 'all',\n",
       " 'that,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'terrifying',\n",
       " 'to',\n",
       " 'me',\n",
       " 'at',\n",
       " 'that',\n",
       " 'age.',\n",
       " '(We',\n",
       " \"aren't\",\n",
       " 'taught',\n",
       " 'sex-ed',\n",
       " 'in',\n",
       " 'school',\n",
       " 'so',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'any',\n",
       " 'better',\n",
       " 'about',\n",
       " 'protection',\n",
       " 'all',\n",
       " 'that.).',\n",
       " 'This',\n",
       " 'was',\n",
       " 'my',\n",
       " 'first',\n",
       " 'every',\n",
       " 'relationship,',\n",
       " 'and',\n",
       " 'according',\n",
       " 'to',\n",
       " 'her,',\n",
       " 'her',\n",
       " '3rd(?)',\n",
       " 'So',\n",
       " 'I',\n",
       " 'was',\n",
       " 'aware',\n",
       " 'she',\n",
       " 'was',\n",
       " 'way',\n",
       " 'more',\n",
       " 'experienced',\n",
       " 'than',\n",
       " 'I',\n",
       " 'was.',\n",
       " 'The',\n",
       " 'incident',\n",
       " 'happened',\n",
       " 'when',\n",
       " 'she',\n",
       " 'invited',\n",
       " 'me',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'the',\n",
       " 'night',\n",
       " 'at',\n",
       " 'her',\n",
       " 'place',\n",
       " '(I',\n",
       " 'was',\n",
       " 'staying',\n",
       " 'in',\n",
       " 'a',\n",
       " 'hotel,',\n",
       " 'in',\n",
       " 'my',\n",
       " 'own',\n",
       " 'room',\n",
       " 'separate',\n",
       " 'from',\n",
       " \"mom's\",\n",
       " 'friend)',\n",
       " 'since',\n",
       " 'her',\n",
       " 'parents',\n",
       " \"weren't\",\n",
       " 'around',\n",
       " '&amp;',\n",
       " 'I',\n",
       " 'agreed.',\n",
       " 'After',\n",
       " 'watching',\n",
       " 'movies',\n",
       " 'and',\n",
       " 'playing',\n",
       " 'games',\n",
       " 'on',\n",
       " 'her',\n",
       " 'computer',\n",
       " 'and',\n",
       " 'such',\n",
       " 'we',\n",
       " 'went',\n",
       " 'to',\n",
       " 'bed.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'tired',\n",
       " 'so',\n",
       " 'just',\n",
       " 'a',\n",
       " 'goodnight',\n",
       " 'peck',\n",
       " 'and',\n",
       " 'sleep.',\n",
       " 'She',\n",
       " 'kept',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'engage',\n",
       " 'in',\n",
       " 'sexual',\n",
       " 'activity',\n",
       " 'but',\n",
       " 'I',\n",
       " 'kept',\n",
       " 'refusing',\n",
       " 'because',\n",
       " 'as',\n",
       " 'said',\n",
       " 'above,',\n",
       " 'I',\n",
       " 'really',\n",
       " 'just',\n",
       " \"didn't\",\n",
       " 'want',\n",
       " 'it.',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'was',\n",
       " 'that',\n",
       " 'until',\n",
       " 'I',\n",
       " 'woke',\n",
       " 'up',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'night',\n",
       " 'for',\n",
       " 'a',\n",
       " 'bathroom',\n",
       " 'break',\n",
       " 'and',\n",
       " 'noticed',\n",
       " 'both',\n",
       " 'our',\n",
       " 'clothes',\n",
       " 'were',\n",
       " 'off',\n",
       " 'and',\n",
       " 'she',\n",
       " 'had',\n",
       " 'her',\n",
       " 'hand',\n",
       " 'on',\n",
       " 'my',\n",
       " 'privates.',\n",
       " 'This',\n",
       " 'obviously',\n",
       " 'freaked',\n",
       " 'me',\n",
       " 'out',\n",
       " 'and',\n",
       " 'i',\n",
       " 'kept',\n",
       " 'questioning',\n",
       " 'her',\n",
       " 'why',\n",
       " 'is',\n",
       " 'her',\n",
       " 'hand',\n",
       " 'there,',\n",
       " 'what',\n",
       " 'is',\n",
       " 'she',\n",
       " 'doing',\n",
       " '&amp;',\n",
       " 'why',\n",
       " 'are',\n",
       " 'my',\n",
       " 'clothes',\n",
       " 'off',\n",
       " '(i',\n",
       " 'slept',\n",
       " 'fully',\n",
       " 'clothed).',\n",
       " 'She',\n",
       " 'just',\n",
       " 'kept..',\n",
       " 'ignoring',\n",
       " 'me.',\n",
       " 'Asking',\n",
       " 'me',\n",
       " 'things',\n",
       " 'like,',\n",
       " '\"why',\n",
       " \"aren't\",\n",
       " 'you',\n",
       " 'excited/hard\"',\n",
       " 'to',\n",
       " '\"let\\'s',\n",
       " 'do',\n",
       " 'it,',\n",
       " \"we're\",\n",
       " 'old',\n",
       " 'enough',\n",
       " 'and',\n",
       " 'long',\n",
       " 'into',\n",
       " 'this',\n",
       " 'relationship\".',\n",
       " 'After',\n",
       " 'I',\n",
       " 'kept',\n",
       " 'refusing',\n",
       " 'i',\n",
       " 'just',\n",
       " 'got',\n",
       " 'up',\n",
       " 'and',\n",
       " 'slept',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'in',\n",
       " 'her',\n",
       " 'living',\n",
       " 'room',\n",
       " 'and',\n",
       " 'left',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'sunrise.',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'speak',\n",
       " 'because',\n",
       " 'it',\n",
       " 'felt',\n",
       " 'so',\n",
       " 'embarrassing',\n",
       " 'that',\n",
       " 'she',\n",
       " 'even',\n",
       " 'saw',\n",
       " 'me',\n",
       " 'naked',\n",
       " '(again,',\n",
       " 'very',\n",
       " 'insecure).',\n",
       " 'After',\n",
       " 'I',\n",
       " 'left',\n",
       " 'she',\n",
       " 'messaged',\n",
       " 'me',\n",
       " 'asking',\n",
       " 'why',\n",
       " 'i',\n",
       " 'acted',\n",
       " 'out',\n",
       " 'the',\n",
       " 'way',\n",
       " 'I',\n",
       " 'did.',\n",
       " 'I',\n",
       " 'replied',\n",
       " 'with',\n",
       " 'what',\n",
       " 'I',\n",
       " 'mentioned',\n",
       " 'earlier,',\n",
       " 'about',\n",
       " 'not',\n",
       " 'wanting',\n",
       " 'it',\n",
       " '&amp;',\n",
       " 'being',\n",
       " 'embarrassed',\n",
       " 'by',\n",
       " 'it.',\n",
       " 'She',\n",
       " 'told',\n",
       " 'me',\n",
       " \"it's\",\n",
       " 'normal',\n",
       " '&amp;',\n",
       " 'all',\n",
       " 'relationships',\n",
       " 'do',\n",
       " 'it..',\n",
       " 'and',\n",
       " 'because',\n",
       " \"I've\",\n",
       " 'never',\n",
       " 'been',\n",
       " 'in',\n",
       " 'one',\n",
       " 'before,',\n",
       " 'I',\n",
       " \"wouldn't\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'couples',\n",
       " 'do(?).',\n",
       " 'I',\n",
       " 'stupidly',\n",
       " 'believed',\n",
       " 'her',\n",
       " 'and',\n",
       " 'apologized',\n",
       " 'alot',\n",
       " '.',\n",
       " 'And',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'alot',\n",
       " 'like',\n",
       " 'by',\n",
       " 'buying',\n",
       " 'gifts',\n",
       " 'and',\n",
       " 'such.',\n",
       " 'Ever',\n",
       " 'since',\n",
       " 'that,',\n",
       " 'she',\n",
       " 'never',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'again..',\n",
       " 'I',\n",
       " 'think.',\n",
       " \"I'm\",\n",
       " 'not',\n",
       " 'sure',\n",
       " 'as',\n",
       " \"I'm\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'heavy',\n",
       " 'sleeper',\n",
       " 'due',\n",
       " 'to',\n",
       " 'some',\n",
       " 'health',\n",
       " 'complications',\n",
       " '(I',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'about',\n",
       " 'at',\n",
       " 'that',\n",
       " 'age).',\n",
       " 'Around',\n",
       " 'age',\n",
       " '18,',\n",
       " 'We',\n",
       " 'broke',\n",
       " 'up.',\n",
       " 'Because',\n",
       " 'she',\n",
       " 'cheated',\n",
       " 'on',\n",
       " 'me,',\n",
       " 'It',\n",
       " 'was',\n",
       " 'heartbreaking',\n",
       " 'and',\n",
       " 'painful',\n",
       " 'for',\n",
       " 'me',\n",
       " 'because',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'long-term',\n",
       " 'up',\n",
       " 'until',\n",
       " 'marriage.',\n",
       " '(I',\n",
       " 'know,',\n",
       " 'naive',\n",
       " 'thinking).',\n",
       " 'It',\n",
       " 'was',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'get',\n",
       " 'over,',\n",
       " 'but',\n",
       " 'eventually',\n",
       " 'I',\n",
       " 'did.',\n",
       " '––',\n",
       " 'At',\n",
       " 'age',\n",
       " '22',\n",
       " 'I',\n",
       " 'met',\n",
       " 'my',\n",
       " 'current',\n",
       " 'girlfriend',\n",
       " 'who',\n",
       " \"I've\",\n",
       " 'known',\n",
       " 'for',\n",
       " 'a',\n",
       " 'few',\n",
       " 'years.',\n",
       " \"I've\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'very',\n",
       " 'low',\n",
       " 'profile',\n",
       " 'private',\n",
       " 'person',\n",
       " 'since',\n",
       " 'I',\n",
       " 'broke',\n",
       " 'up',\n",
       " 'with',\n",
       " 'my',\n",
       " 'ex',\n",
       " 'at',\n",
       " '18',\n",
       " 'and',\n",
       " 'closed',\n",
       " 'myself',\n",
       " 'off',\n",
       " 'emotionally.',\n",
       " 'However,',\n",
       " 'since',\n",
       " 'being',\n",
       " 'with',\n",
       " 'my',\n",
       " 'now-girlfriend',\n",
       " '(we',\n",
       " 'recently',\n",
       " 'had',\n",
       " 'our',\n",
       " 'first',\n",
       " 'anniversary,',\n",
       " 'yay)',\n",
       " \"I've\",\n",
       " 'been',\n",
       " 'able',\n",
       " 'to',\n",
       " 'emotionally',\n",
       " 'open',\n",
       " 'up',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'more.',\n",
       " \"She's\",\n",
       " 'shared',\n",
       " 'alot',\n",
       " 'about',\n",
       " 'her',\n",
       " 'life',\n",
       " 'to',\n",
       " 'me..',\n",
       " 'both',\n",
       " 'ups',\n",
       " 'and',\n",
       " 'downs',\n",
       " '&amp;',\n",
       " 'even',\n",
       " 'the',\n",
       " 'serious',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'abuse',\n",
       " 'she',\n",
       " 'dealt',\n",
       " 'with',\n",
       " 'and',\n",
       " 'all.',\n",
       " \"I've\",\n",
       " 'recently',\n",
       " 'come',\n",
       " 'to',\n",
       " 'an',\n",
       " 'understanding',\n",
       " '(better',\n",
       " 'late',\n",
       " 'than',\n",
       " 'never..)',\n",
       " 'that',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'me',\n",
       " 'when',\n",
       " 'I',\n",
       " 'was',\n",
       " '15',\n",
       " 'with',\n",
       " 'my',\n",
       " 'ex',\n",
       " 'was',\n",
       " 'NOT',\n",
       " 'normal',\n",
       " 'and',\n",
       " 'was',\n",
       " 'actually',\n",
       " 'sexual',\n",
       " 'abuse/assault.',\n",
       " 'Since',\n",
       " 'then',\n",
       " \"I've\",\n",
       " 'been',\n",
       " 'spiralling',\n",
       " 'down',\n",
       " 'because',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'so',\n",
       " 'disgusted',\n",
       " 'by',\n",
       " 'it.',\n",
       " 'To',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'victim',\n",
       " 'just',\n",
       " 'idk..',\n",
       " 'hurt',\n",
       " 'me?',\n",
       " 'I',\n",
       " \"couldn't\",\n",
       " 'stop',\n",
       " 'thinking',\n",
       " 'about',\n",
       " 'it',\n",
       " 'to',\n",
       " 'the',\n",
       " 'point',\n",
       " 'I',\n",
       " 'just',\n",
       " 'shutdown.',\n",
       " \"It's\",\n",
       " 'been',\n",
       " 'on',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'for',\n",
       " 'weeks',\n",
       " 'now.',\n",
       " 'My',\n",
       " 'girlfriend',\n",
       " 'has',\n",
       " 'noticed',\n",
       " 'my',\n",
       " 'rapid',\n",
       " 'mood',\n",
       " 'shift',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'worried',\n",
       " 'about',\n",
       " 'me',\n",
       " 'because',\n",
       " 'I',\n",
       " \"haven't\",\n",
       " 'told',\n",
       " 'her',\n",
       " 'why.',\n",
       " \"I'm\",\n",
       " 'looking',\n",
       " 'for',\n",
       " 'just',\n",
       " 'advice',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'her',\n",
       " 'about',\n",
       " 'it?',\n",
       " 'My',\n",
       " 'biggest',\n",
       " 'fear',\n",
       " 'is',\n",
       " 'that',\n",
       " \"she'll\",\n",
       " 'think',\n",
       " 'of',\n",
       " 'me',\n",
       " 'as',\n",
       " 'less',\n",
       " 'of',\n",
       " 'a',\n",
       " 'man',\n",
       " 'because',\n",
       " 'I',\n",
       " 'let',\n",
       " 'it',\n",
       " 'happen..',\n",
       " \"I'm\",\n",
       " 'just',\n",
       " 'a',\n",
       " 'mess.',\n",
       " 'Hopefully',\n",
       " 'this',\n",
       " 'post',\n",
       " 'somewhat',\n",
       " 'made',\n",
       " 'sense',\n",
       " 'to',\n",
       " 'someone.',\n",
       " 'I',\n",
       " 'appreciate',\n",
       " 'any',\n",
       " 'talks',\n",
       " 'of',\n",
       " 'advice',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'approach',\n",
       " 'this',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'her.',\n",
       " 'Thank',\n",
       " 'you.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpost.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
